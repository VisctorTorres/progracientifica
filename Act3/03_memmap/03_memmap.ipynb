{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Scientific Programming in Python</h1>\n",
    "<h2 align=\"center\">Topic 3: Handling Very Large Arrays, Memory Mappings</h2> \n",
    "\n",
    "\n",
    "_Notebook created by Mart√≠n Villanueva - `martin.villanueva@usm.cl` - DI UTFSM - April 2017._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"\n",
    "    Recursively finds size of objects\n",
    "    ref: https://goshippo.com/blog/measure-real-size-any-python-object/\n",
    "    \"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [0.- Memory Profiling](#mem)\n",
    "* [1.- Very Large Arrays](#larray)\n",
    "* [2.- NumPy's Memory Mappings](#memmap)\n",
    "* [3.- HDF5 and h5py](#hdf5)\n",
    "* [4.- Sparse Matrices](#sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='mem' />\n",
    "## Memory Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the `memit` magic, you have to install the `memory_profiler` module, with one of this commands:\n",
    "```Bash\n",
    "conda install memory_profiler\n",
    "pip install memory_profiler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%memit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure memory usage of a Python statement\n",
    "\n",
    "Usage, in line mode:\n",
    "```Python\n",
    "  %memit [-r<R>t<T>i<I>] statement\n",
    "```\n",
    "\n",
    "\n",
    "Usage, in cell mode:\n",
    "```Python\n",
    "  %%memit [-r<R>t<T>i<I>] setup_code\n",
    "  code...\n",
    "  code...\n",
    "```\n",
    "\n",
    "\n",
    "This function can be used both as a line and cell magic:\n",
    "\n",
    "- In line mode you can measure a single-line statement (though multiple\n",
    "  ones can be chained with using semicolons).\n",
    "\n",
    "- In cell mode, the statement in the first line is used as setup code\n",
    "  (executed but not measured) and the body of the cell is measured.\n",
    "  The cell body has access to any variables created in the setup code.\n",
    "\n",
    "Options:\n",
    "\n",
    "-r<R>: repeat the loop iteration <R> times and take the best result.\n",
    "Default: 1\n",
    "\n",
    "-t<T>: timeout after <T> seconds. Default: None\n",
    "\n",
    "-i<I>: Get time information at an interval of I times per second.\n",
    "    Defaults to 0.1 so that there is ten measurements per second.\n",
    "\n",
    "-c: If present, add the memory usage of any children process to the report.\n",
    "\n",
    "-o: If present, return a object containing memit run details\n",
    "\n",
    "-q: If present, be quiet and do not output a result.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "::\n",
    "```Python\n",
    "  In [1]: %memit range(10000)\n",
    "  peak memory: 21.42 MiB, increment: 0.41 MiB\n",
    "\n",
    "  In [2]: %memit range(1000000)\n",
    "  peak memory: 52.10 MiB, increment: 31.08 MiB\n",
    "\n",
    "  In [3]: %%memit l=range(1000000)\n",
    "     ...: len(l)\n",
    "     ...:\n",
    "  peak memory: 52.14 MiB, increment: 0.08 MiB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 50.78 MiB, increment: 0.54 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit  np.ones((100,100), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='larray' />\n",
    "## 1.- Very Large Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.arrays` are meant __to live in memory__. If you want to work with matrices __larger than your RAM__, you have to work around that. There are at least two approaches you can follow:\n",
    "\n",
    "1. __Try a more efficient matrix representation__ that exploits any special structure that your matrices have. For example, there are efficient data structures for __sparse matrices__ (matrices with lots of zeros), like `scipy.sparse.csc_matrix`.\n",
    "2. __Modify your algorithm to work on submatrices__. You can read from disk only the __matrix blocks__ that are currently being used in computations. Algorithms designed to run on clusters usually work blockwise, since the data is scatted across different computers, and passed by only when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='memmap' />\n",
    "## 2.- NumPy's Memory Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important features of __Memory Mappings__:\n",
    "\n",
    "1. Memory mapping lets you work with huge arrays almost as if they were regular arrays.\n",
    "2. Python code that accepts a NumPy array as input will also accept a memmap array.\n",
    "3. We need to ensure that the array is used __efficiently__, i.e, the array is never loaded as a whole.\n",
    "4. This method is not the most adapted for long-term storage of data and data sharing (__HDF5 is it!__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Let's create a memory-mapped array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.core.memmap.memmap'>\n"
     ]
    }
   ],
   "source": [
    "nrows, ncols = 100, 1000000\n",
    "f = np.memmap('memmapped.dat', dtype=np.float32, mode='w+', shape=(nrows, ncols))\n",
    "\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Let's feed the array with random values, one column at a time because our system's memory is limited! (__Reducing garbage collector work__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 438.49 MiB, increment: 387.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "for i in range(nrows):\n",
    "    tmp = np.random.random(ncols)\n",
    "    f[i,:] = tmp\n",
    "    del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 688.25 MiB, increment: 248.45 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "np.random.random((nrows,ncols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the last column of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56767881  0.75050998  0.54165035 ...,  0.66141927  0.39430636\n",
      "  0.5874598 ]\n"
     ]
    }
   ],
   "source": [
    "x = f[-1,:]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Now, we flush memory changes to disk by deleting the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44641563  0.7079947   0.54792404 ...,  0.51614118  0.71634042\n",
      "   0.01647442]\n",
      " [ 0.71352762  0.0656995   0.55586261 ...,  0.99392563  0.93927854\n",
      "   0.13646419]\n",
      " [ 0.87408727  0.42331061  0.4700405  ...,  0.05731088  0.54089528\n",
      "   0.06342366]\n",
      " ..., \n",
      " [ 0.46890846  0.43627045  0.91058242 ...,  0.20603104  0.89088571\n",
      "   0.25193346]\n",
      " [ 0.15489034  0.40641561  0.33165485 ...,  0.01800282  0.37709102\n",
      "   0.16161576]\n",
      " [ 0.56767881  0.75050998  0.54165035 ...,  0.66141927  0.39430636\n",
      "   0.5874598 ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3056c44daef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "print(f)\n",
    "del f\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Reading a memory-mapped array from disk involves the same `memmap` function. The `data type` and the `shape` need to be specified again, as this information is not stored in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 235.96 MiB, increment: 0.04 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit \n",
    "f = np.memmap('memmapped.dat', dtype=np.float32, shape=(nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 240.76 MiB, increment: 4.79 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "np.array_equal(f[-1,:],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.core.memmap.memmap'>\n"
     ]
    }
   ],
   "source": [
    "print(type(f[-1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hdf5' />\n",
    "## 3.- HDF5 and `h5py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Why HD5 instead of native NumPy's memmap:__\n",
    "\n",
    "1. __HDF5__ (_Hierarchical Data Format version 5_) is an open format specification, i.e, language independet. Can be used with Python, Matlab, R, C, Java, etc.\n",
    "2. HDF5 not only implement _memory mappings_ but also provides a POSIX-like hierarchy to organize different arrays.\n",
    "3. It is more versatile than NumPy's memmap. The latter (`numpy.memmap`) stores the array in binary form on the hard-disk using contiguous blocks of memory. The first (`hdf5`) work with __chunks__ (which are atomic objects) organized in a __B-tree__ structure. _The content of a chunk is contiguously stored in the hard-disk_.\n",
    "\n",
    "There are two libraries to work with `HDF5` in Python: `PyTables` and `h5py`. We use the second one since it is more lightweight and more adapted than `PyTables`in some situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.- Let's create a new empty `HDF5` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File(\"myfile.h5\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- We create a `1000x1000` array (just the memory mapping in fact...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows = 100\n",
    "ncols = 1000000\n",
    "dset = f.create_dataset(\"dataset\", (nrows,ncols), dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "(100, 1000000)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(type(dset))\n",
    "print(dset.shape)\n",
    "print(dset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- We now fill it with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 239.63 MiB, increment: 1.50 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "for i in range(nrows):\n",
    "    tmp = np.random.random(ncols)\n",
    "    dset[i,:] = tmp\n",
    "    del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 847.91 MiB, increment: 608.28 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "a = np.random.random((nrows,ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 0.69053257  0.41006225  0.13490033 ...,  0.35091314  0.43940765\n",
      "  0.8976711 ]\n"
     ]
    }
   ],
   "source": [
    "# we will store the last row\n",
    "x = dset[-1,:]\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About groups and hierarchical organization...\n",
    "\n",
    "An HDF5 file is a container for two kinds of objects: datasets, which are array-like collections of data, and groups, which are folder-like containers that hold datasets and other groups. The most fundamental thing to remember when using h5py is:\n",
    "\n",
    "__Groups work like folders, and datasets work like NumPy arrays!__\n",
    "\n",
    "<img src='data/hdf5.jpg' style=\"width: 600px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The File object we created is itself a group, in this case the root group\n",
    "print( f.name )\n",
    "print( dset.name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets create a subgroup\n",
    "f.create_group(\"subgroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets create a new dataset inside this subgroup\n",
    "dset2 = f.create_dataset('subgroup/dataset2', (10,), dtype='i')\n",
    "print(dset2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieving datasets/arrays from hdf5 file\n",
    "print( f[\"dataset\"] )\n",
    "print( f[\"subgroup/dataset2\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets re-open the `hdf5` file, and compare the last row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%memit\n",
    "f = h5py.File(\"myfile.h5\", \"r+\")\n",
    "dset = f[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%memit\n",
    "print(np.array_equal(dset[-1,:],x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical example: Huge matrix multiplication.\n",
    "\n",
    "Supose we want to perform the dot product between $A \\in \\mathbb{R}^{m\\times n}$ and $b \\in \\mathbb{R}^n$, such that $A$ doesn't fit in the available RAM memory. __How can we solve it? R: Memory Mappings__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We first create the hdf5 file, and add a dataset/array\n",
    "nrows = 1000\n",
    "ncols = 100000\n",
    "\n",
    "f = h5py.File(\"test.h5\", \"w\")\n",
    "dset = f.create_dataset(\"array\", (nrows,ncols), dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%memit\n",
    "tmp = np.random.random((nrows,ncols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the array with random values, without loading the whole array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%memit -r 2\n",
    "for i in range(nrows):\n",
    "    tmp = np.random.random(ncols)\n",
    "    dset[i,:] = tmp\n",
    "    del tmp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the $b$ array and a $c$ empty array to store the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.random.random(ncols)\n",
    "c = np.empty(ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform dot product without loading $A$ (completely) into main memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%memit\n",
    "for i in range(nrows):\n",
    "    c[i] = np.dot(dset[i,:],b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for i in range(nrows):\n",
    "    c[i] = np.dot(dset[i,:],b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save memory, but pay with overhead in time (_The time needed to retreive each row in `dset` into main memory_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.random((nrows,ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "np.dot(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The performance of _memory mapping_ approach can be improved by multiplying more than one row at each iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the datasets: Improving I/O performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An `hdf5` dataset created with the default settings will be contiguous; i.e, laid out on disk in __traditional C order__. \n",
    "* Datasets may also be created using `hdf5`‚Äôs _chunked storage layout_. This means the dataset is __divided up into regularly-sized pieces__ which are stored randomly on disk, and indexed using a __B-tree__.\n",
    "\n",
    "__ We a single data of a chunk is indexed, the whole chunk is loaded (chunks are atomic)!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To enable chunked storage, set the keyword chunks to a tuple indicating the chunk shape\n",
    "f = h5py.File(\"test2.h5\", \"w\")\n",
    "dset = f.create_dataset(\"chunked\", (1000, 1000), chunks=(100, 100), dtype='f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be read and written now in blocks with shape (100,100): `dset[0:100,0:100]` for example.\n",
    "\n",
    "__Trade-off:__ There is a trade-off between __many small chunks__ (large overhead due to managing lots of chunks) and a __few large chunks__ (inefficient disk I/O). The chunk size is recommended __to be smaller than 1 MB__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sparse' />\n",
    "## 4.- Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __sparse array__ is an array in which __most of the elements have the value 0__. The occurrence of zero-value elements in a large array is inefficient for both __computation__ and __storage__. An array in which there is a large number of zero elements is referred to as being sparse.\n",
    "\n",
    "Such matrices appear naturally in many applications:\n",
    "1. Finite Element Methods (PDEs solving method).\n",
    "2. Finite Differences Methods (PDEs solving method).\n",
    "3. Discrete Wavelet Transforms (√Å trous)\n",
    "4. Machine Learning: SVM and support vectors.\n",
    "5. Machine Learning: Neural Networks (FeedForward) Weights.\n",
    "6. Etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sparse matrix of the given `shape` and `density` with _randomly distributed values_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrows = 20\n",
    "ncols = 20\n",
    "A = sp.sparse.random(nrows, ncols, density=0.1, format='csr')\n",
    "B = A.toarray()\n",
    "print(type(A))\n",
    "print(type(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the matrix structure with `matplotlib.pyplot.matshow` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.matshow(B, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sparse matrices are stored in memory?\n",
    "\n",
    "__Answer:__ With _linkded lists_! (We just store the needed values in such lists). There are seven available sparse matrix types/formats:\n",
    "\n",
    "1. __csc_matrix__: Compressed Sparse Column format\n",
    "2. __csr_matrix__: Compressed Sparse Row format\n",
    "3. __bsr_matrix__: Block Sparse Row format\n",
    "4. __lil_matrix__: List of Lists format\n",
    "5. __dok_matrix__: Dictionary of Keys format\n",
    "6. __coo_matrix__: COOrdinate format (aka IJV, triplet format)\n",
    "7. __dia_matrix__: DIAgonal format\n",
    "\n",
    "#### CSR example\n",
    "\n",
    "<img src='data/csr.png' style=\"width: 600px;\">\n",
    "\n",
    "For more info see [Scipy Documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html)\n",
    "\n",
    "__Important note:__ Despite their similarity to NumPy arrays, it is strongly discouraged to use NumPy functions directly on these matrices because NumPy may not properly convert them for computations, leading to unexpected (and incorrect) results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical example again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows = 1000\n",
    "ncols = 100000\n",
    "A = sp.sparse.random(nrows, ncols, density=0.1, format='csr')\n",
    "B = A.toarray()\n",
    "b = np.random.random(ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Size of A:\",get_size(A),\"Bytes\")\n",
    "print(\"Size of B:\",get_size(B),\"Bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit A.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit B.dot(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
